[
    {
        "label": "time",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "time",
        "description": "time",
        "detail": "time",
        "documentation": {}
    },
    {
        "label": "json",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "json",
        "description": "json",
        "detail": "json",
        "documentation": {}
    },
    {
        "label": "os",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "os",
        "description": "os",
        "detail": "os",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "psutil",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "psutil",
        "description": "psutil",
        "detail": "psutil",
        "documentation": {}
    },
    {
        "label": "SequenceMatcher",
        "importPath": "difflib",
        "description": "difflib",
        "isExtraImport": true,
        "detail": "difflib",
        "documentation": {}
    },
    {
        "label": "ollama",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "ollama",
        "description": "ollama",
        "detail": "ollama",
        "documentation": {}
    },
    {
        "label": "threading",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "threading",
        "description": "threading",
        "detail": "threading",
        "documentation": {}
    },
    {
        "label": "TfidfVectorizer",
        "importPath": "sklearn.feature_extraction.text",
        "description": "sklearn.feature_extraction.text",
        "isExtraImport": true,
        "detail": "sklearn.feature_extraction.text",
        "documentation": {}
    },
    {
        "label": "cosine_similarity",
        "importPath": "sklearn.metrics.pairwise",
        "description": "sklearn.metrics.pairwise",
        "isExtraImport": true,
        "detail": "sklearn.metrics.pairwise",
        "documentation": {}
    },
    {
        "label": "LLMBenchmark",
        "importPath": "benchmark",
        "description": "benchmark",
        "isExtraImport": true,
        "detail": "benchmark",
        "documentation": {}
    },
    {
        "label": "create_visualizations",
        "importPath": "visualization",
        "description": "visualization",
        "isExtraImport": true,
        "detail": "visualization",
        "documentation": {}
    },
    {
        "label": "load_all_benchmarks",
        "importPath": "tasks",
        "description": "tasks",
        "isExtraImport": true,
        "detail": "tasks",
        "documentation": {}
    },
    {
        "label": "pandas",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "pandas",
        "description": "pandas",
        "detail": "pandas",
        "documentation": {}
    },
    {
        "label": "matplotlib.pyplot",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "matplotlib.pyplot",
        "description": "matplotlib.pyplot",
        "detail": "matplotlib.pyplot",
        "documentation": {}
    },
    {
        "label": "seaborn",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "seaborn",
        "description": "seaborn",
        "detail": "seaborn",
        "documentation": {}
    },
    {
        "label": "numpy",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "numpy",
        "description": "numpy",
        "detail": "numpy",
        "documentation": {}
    },
    {
        "label": "pi",
        "importPath": "math",
        "description": "math",
        "isExtraImport": true,
        "detail": "math",
        "documentation": {}
    },
    {
        "label": "runpy",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "runpy",
        "description": "runpy",
        "detail": "runpy",
        "documentation": {}
    },
    {
        "label": "annotations",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "site",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "site",
        "description": "site",
        "detail": "site",
        "documentation": {}
    },
    {
        "label": "sys",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "sys",
        "description": "sys",
        "detail": "sys",
        "documentation": {}
    },
    {
        "label": "LLMBenchmark",
        "kind": 6,
        "importPath": "benchmark_framework.benchmark",
        "description": "benchmark_framework.benchmark",
        "peekOfCode": "class LLMBenchmark:\n    def __init__(self,models,tasks):\n        self.models = models\n        self.tasks = tasks\n        self.results = {}\n        os.makedirs('results', exist_ok=True)\n    def run_benchmarks(self):\n        for model in self.models:\n            print(f\"Benchmarking {model}...\")\n            self.results[model] = {}",
        "detail": "benchmark_framework.benchmark",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "benchmark_framework.run_benchmark",
        "description": "benchmark_framework.run_benchmark",
        "peekOfCode": "def main():\n    # Creat results directory\n    os.makedirs(\"results\",exist_ok=True)\n    # Define models to benchmark\n    models = ['phi', 'mistral', 'llama3:8b']\n    print (\"Starting LLM benchmarking\")\n    start_time = time.time()\n    # Load all benchmark tasks\n    tasks = load_all_benchmarks()\n    # Initialize the LLMBenchmark class with models and tasks",
        "detail": "benchmark_framework.run_benchmark",
        "documentation": {}
    },
    {
        "label": "create_qa_benchmark",
        "kind": 2,
        "importPath": "benchmark_framework.tasks",
        "description": "benchmark_framework.tasks",
        "peekOfCode": "def create_qa_benchmark(questions_file):\n    \"\"\"\n    Load and format question-answering benchmark tasks.\n    Args:\n        file_path (str): Path to the JSON file containing QA pairs\n    Returns:\n        list: List of formatted benchmark tasks\n    \"\"\"\n    with open(questions_file, 'r') as file:\n        questions = json.load(file)",
        "detail": "benchmark_framework.tasks",
        "documentation": {}
    },
    {
        "label": "create_code_benchmark",
        "kind": 2,
        "importPath": "benchmark_framework.tasks",
        "description": "benchmark_framework.tasks",
        "peekOfCode": "def create_code_benchmark(problems_file):\n    \"\"\"\n    Load and format code generation benchmark tasks.\n    Args:\n        file_path (str): Path to the JSON file containing coding problems\n    Returns:\n        list: List of formatted benchmark tasks\n    \"\"\"\n    with open(problems_file, 'r') as file:\n        problems = json.load(file)",
        "detail": "benchmark_framework.tasks",
        "documentation": {}
    },
    {
        "label": "create_summarization_benchmark",
        "kind": 2,
        "importPath": "benchmark_framework.tasks",
        "description": "benchmark_framework.tasks",
        "peekOfCode": "def create_summarization_benchmark(article_file):\n    \"\"\"\n    Load and format text summarization benchmark tasks.\n    Args:\n        file_path (str): Path to the JSON file containing text to summarize\n    Returns:\n        list: List of formatted benchmark tasks\n    \"\"\"\n    with open(article_file, 'r') as file:\n        articles = json.load(file)",
        "detail": "benchmark_framework.tasks",
        "documentation": {}
    },
    {
        "label": "create_reasoning_benchmark",
        "kind": 2,
        "importPath": "benchmark_framework.tasks",
        "description": "benchmark_framework.tasks",
        "peekOfCode": "def create_reasoning_benchmark(contexts_file):\n    \"\"\"\n    Load and format reasoning benchmark tasks.\n    Args:\n        file_path (str): Path to the JSON file containing reasoning problems\n    Returns:\n        list: List of formatted benchmark tasks\n    \"\"\"\n    with open(contexts_file, 'r') as file:\n        context = json.load(file)",
        "detail": "benchmark_framework.tasks",
        "documentation": {}
    },
    {
        "label": "load_all_benchmarks",
        "kind": 2,
        "importPath": "benchmark_framework.tasks",
        "description": "benchmark_framework.tasks",
        "peekOfCode": "def load_all_benchmarks(data_dir=\"data\"):\n    \"\"\"\n    Load all benchmark tasks from the data directory.\n    Args:\n        data_dir (str): Path to directory containing benchmark data files\n    Returns:\n        dict: Dictionary mapping task types to lists of benchmark tasks\n    \"\"\"\n    benchmarks = {}\n    benchmark_functions = {",
        "detail": "benchmark_framework.tasks",
        "documentation": {}
    },
    {
        "label": "set_plotting_style",
        "kind": 2,
        "importPath": "benchmark_framework.visualization",
        "description": "benchmark_framework.visualization",
        "peekOfCode": "def set_plotting_style():\n    sns.set(style=\"whitegrid\")\n    sns.set_palette(TASK_COLORS)\n    plt.rcParams.update({\n        'figure.figsize': (10, 6),\n        'axes.titlesize': 16,\n        'axes.labelsize': 14,\n        'xtick.labelsize': 12,\n        'ytick.labelsize': 12\n    })",
        "detail": "benchmark_framework.visualization",
        "documentation": {}
    },
    {
        "label": "create_visualizations",
        "kind": 2,
        "importPath": "benchmark_framework.visualization",
        "description": "benchmark_framework.visualization",
        "peekOfCode": "def create_visualizations(summary, results_dir='results/plots'):\n    os.makedirs(results_dir, exist_ok=True)\n    set_plotting_style()\n    df = flatten_summary(summary)\n    create_bar_chart(df, 'avg_score', 'Performance Comparison', results_dir)\n    create_bar_chart(df, 'avg_latency', 'Latency Comparison', results_dir)\n    create_bar_chart(df, 'avg_memory_kb', 'Memory Usage Comparison', results_dir)\n    create_bar_chart(df, 'tokens_per_second', 'Tokens per Second Comparison', results_dir)\n    create_performance_dashboard(df, results_dir)\n    create_performance_vs_speed_scatter(df, results_dir)",
        "detail": "benchmark_framework.visualization",
        "documentation": {}
    },
    {
        "label": "flatten_summary",
        "kind": 2,
        "importPath": "benchmark_framework.visualization",
        "description": "benchmark_framework.visualization",
        "peekOfCode": "def flatten_summary(summary):\n    rows = []\n    for model, tasks in summary.items():\n        for task, metrics in tasks.items():\n            row = {'Model': model, 'Task': task}\n            row.update(metrics)\n            rows.append(row)\n    return pd.DataFrame(rows)\ndef create_bar_chart(df, metric, title, results_dir):\n    plt.figure(figsize=(10, 6))",
        "detail": "benchmark_framework.visualization",
        "documentation": {}
    },
    {
        "label": "create_bar_chart",
        "kind": 2,
        "importPath": "benchmark_framework.visualization",
        "description": "benchmark_framework.visualization",
        "peekOfCode": "def create_bar_chart(df, metric, title, results_dir):\n    plt.figure(figsize=(10, 6))\n    sns.barplot(data=df, x=\"Model\", y=metric, hue=\"Task\", palette=TASK_COLORS)\n    plt.title(title)\n    plt.ylabel(metric.replace(\"_\", \" \").title())\n    plt.xlabel(\"Model\")\n    plt.legend(title=\"Task\")\n    for bar in plt.gca().patches:\n        height = bar.get_height()\n        plt.gca().annotate(f\"{height:.2f}\", (bar.get_x() + bar.get_width()/2., height),",
        "detail": "benchmark_framework.visualization",
        "documentation": {}
    },
    {
        "label": "create_performance_dashboard",
        "kind": 2,
        "importPath": "benchmark_framework.visualization",
        "description": "benchmark_framework.visualization",
        "peekOfCode": "def create_performance_dashboard(df, results_dir):\n    fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n    metrics = ['avg_score', 'avg_latency', 'avg_memory_kb']\n    titles = ['Score', 'Latency (s)', 'Memory (MB)']\n    for ax, metric, title in zip(axes, metrics, titles):\n        sns.barplot(data=df, x='Model', y=metric, hue='Task', ax=ax, palette=TASK_COLORS)\n        ax.set_title(title)\n        ax.set_xlabel('')\n        for bar in ax.patches:\n            height = bar.get_height()",
        "detail": "benchmark_framework.visualization",
        "documentation": {}
    },
    {
        "label": "create_performance_vs_speed_scatter",
        "kind": 2,
        "importPath": "benchmark_framework.visualization",
        "description": "benchmark_framework.visualization",
        "peekOfCode": "def create_performance_vs_speed_scatter(df, results_dir):\n    plt.figure(figsize=(10, 6))\n    sns.scatterplot(data=df, x=\"avg_latency\", y=\"avg_score\", hue=\"Model\", style=\"Task\", s=100, palette=TASK_COLORS)\n    plt.title(\"Performance vs Latency\")\n    plt.xlabel(\"Latency (s)\")\n    plt.ylabel(\"Score\")\n    plt.tight_layout()\n    plt.savefig(os.path.join(results_dir, \"performance_vs_latency_scatter.png\"))\n    plt.close()\ndef create_enhanced_radar_chart(df, results_dir):",
        "detail": "benchmark_framework.visualization",
        "documentation": {}
    },
    {
        "label": "create_enhanced_radar_chart",
        "kind": 2,
        "importPath": "benchmark_framework.visualization",
        "description": "benchmark_framework.visualization",
        "peekOfCode": "def create_enhanced_radar_chart(df, results_dir):\n    metrics = ['avg_score', 'avg_latency', 'avg_memory_kb']\n    models = df['Model'].unique()\n    task_order = ['qa', 'code', 'summarization', 'reasoning']\n    for model in models:\n        radar_data = df[df['Model'] == model].set_index('Task')[metrics]\n        radar_data = radar_data.reindex(task_order).fillna(0.0)\n        radar_data = radar_data.apply(lambda x: (x - x.min()) / (x.max() - x.min() + 1e-8), axis=0)\n        angles = [n / float(len(metrics)) * 2 * pi for n in range(len(metrics))]\n        angles += angles[:1]",
        "detail": "benchmark_framework.visualization",
        "documentation": {}
    },
    {
        "label": "create_enhanced_heatmap",
        "kind": 2,
        "importPath": "benchmark_framework.visualization",
        "description": "benchmark_framework.visualization",
        "peekOfCode": "def create_enhanced_heatmap(summary, results_dir):\n    rows = []\n    for model, task_dict in summary.items():\n        for task, metrics in task_dict.items():\n            row = {'Model': model, 'Task': task, **metrics}\n            rows.append(row)\n    df = pd.DataFrame(rows)\n    pivot_score = df.pivot(index=\"Model\", columns=\"Task\", values=\"avg_score\")\n    pivot_latency = df.pivot(index=\"Model\", columns=\"Task\", values=\"avg_latency\")\n    plt.figure(figsize=(10, 6))",
        "detail": "benchmark_framework.visualization",
        "documentation": {}
    },
    {
        "label": "TASK_COLORS",
        "kind": 5,
        "importPath": "benchmark_framework.visualization",
        "description": "benchmark_framework.visualization",
        "peekOfCode": "TASK_COLORS = ['#332288', '#88CCEE', '#44AA99', '#DDCC77']  # 4-color safe palette\ndef set_plotting_style():\n    sns.set(style=\"whitegrid\")\n    sns.set_palette(TASK_COLORS)\n    plt.rcParams.update({\n        'figure.figsize': (10, 6),\n        'axes.titlesize': 16,\n        'axes.labelsize': 14,\n        'xtick.labelsize': 12,\n        'ytick.labelsize': 12",
        "detail": "benchmark_framework.visualization",
        "documentation": {}
    },
    {
        "label": "bin_dir",
        "kind": 5,
        "importPath": "venv.Scripts.activate_this",
        "description": "venv.Scripts.activate_this",
        "peekOfCode": "bin_dir = os.path.dirname(abs_file)\nbase = bin_dir[: -len('Scripts') - 1]  # strip away the bin part from the __file__, plus the path separator\n# prepend bin to PATH (this file is inside the bin directory)\nos.environ[\"PATH\"] = os.pathsep.join([bin_dir, *os.environ.get(\"PATH\", \"\").split(os.pathsep)])\nos.environ[\"VIRTUAL_ENV\"] = base  # virtual env is right above bin directory\nos.environ[\"VIRTUAL_ENV_PROMPT\"] = '' or os.path.basename(base)\n# add the virtual environments libraries to the host python import mechanism\nprev_length = len(sys.path)\nfor lib in '..\\\\Lib\\\\site-packages'.split(os.pathsep):\n    path = os.path.realpath(os.path.join(bin_dir, lib))",
        "detail": "venv.Scripts.activate_this",
        "documentation": {}
    },
    {
        "label": "base",
        "kind": 5,
        "importPath": "venv.Scripts.activate_this",
        "description": "venv.Scripts.activate_this",
        "peekOfCode": "base = bin_dir[: -len('Scripts') - 1]  # strip away the bin part from the __file__, plus the path separator\n# prepend bin to PATH (this file is inside the bin directory)\nos.environ[\"PATH\"] = os.pathsep.join([bin_dir, *os.environ.get(\"PATH\", \"\").split(os.pathsep)])\nos.environ[\"VIRTUAL_ENV\"] = base  # virtual env is right above bin directory\nos.environ[\"VIRTUAL_ENV_PROMPT\"] = '' or os.path.basename(base)\n# add the virtual environments libraries to the host python import mechanism\nprev_length = len(sys.path)\nfor lib in '..\\\\Lib\\\\site-packages'.split(os.pathsep):\n    path = os.path.realpath(os.path.join(bin_dir, lib))\n    site.addsitedir(path.decode(\"utf-8\") if '' else path)",
        "detail": "venv.Scripts.activate_this",
        "documentation": {}
    },
    {
        "label": "os.environ[\"PATH\"]",
        "kind": 5,
        "importPath": "venv.Scripts.activate_this",
        "description": "venv.Scripts.activate_this",
        "peekOfCode": "os.environ[\"PATH\"] = os.pathsep.join([bin_dir, *os.environ.get(\"PATH\", \"\").split(os.pathsep)])\nos.environ[\"VIRTUAL_ENV\"] = base  # virtual env is right above bin directory\nos.environ[\"VIRTUAL_ENV_PROMPT\"] = '' or os.path.basename(base)\n# add the virtual environments libraries to the host python import mechanism\nprev_length = len(sys.path)\nfor lib in '..\\\\Lib\\\\site-packages'.split(os.pathsep):\n    path = os.path.realpath(os.path.join(bin_dir, lib))\n    site.addsitedir(path.decode(\"utf-8\") if '' else path)\nsys.path[:] = sys.path[prev_length:] + sys.path[0:prev_length]\nsys.real_prefix = sys.prefix",
        "detail": "venv.Scripts.activate_this",
        "documentation": {}
    },
    {
        "label": "os.environ[\"VIRTUAL_ENV\"]",
        "kind": 5,
        "importPath": "venv.Scripts.activate_this",
        "description": "venv.Scripts.activate_this",
        "peekOfCode": "os.environ[\"VIRTUAL_ENV\"] = base  # virtual env is right above bin directory\nos.environ[\"VIRTUAL_ENV_PROMPT\"] = '' or os.path.basename(base)\n# add the virtual environments libraries to the host python import mechanism\nprev_length = len(sys.path)\nfor lib in '..\\\\Lib\\\\site-packages'.split(os.pathsep):\n    path = os.path.realpath(os.path.join(bin_dir, lib))\n    site.addsitedir(path.decode(\"utf-8\") if '' else path)\nsys.path[:] = sys.path[prev_length:] + sys.path[0:prev_length]\nsys.real_prefix = sys.prefix\nsys.prefix = base",
        "detail": "venv.Scripts.activate_this",
        "documentation": {}
    },
    {
        "label": "os.environ[\"VIRTUAL_ENV_PROMPT\"]",
        "kind": 5,
        "importPath": "venv.Scripts.activate_this",
        "description": "venv.Scripts.activate_this",
        "peekOfCode": "os.environ[\"VIRTUAL_ENV_PROMPT\"] = '' or os.path.basename(base)\n# add the virtual environments libraries to the host python import mechanism\nprev_length = len(sys.path)\nfor lib in '..\\\\Lib\\\\site-packages'.split(os.pathsep):\n    path = os.path.realpath(os.path.join(bin_dir, lib))\n    site.addsitedir(path.decode(\"utf-8\") if '' else path)\nsys.path[:] = sys.path[prev_length:] + sys.path[0:prev_length]\nsys.real_prefix = sys.prefix\nsys.prefix = base",
        "detail": "venv.Scripts.activate_this",
        "documentation": {}
    },
    {
        "label": "prev_length",
        "kind": 5,
        "importPath": "venv.Scripts.activate_this",
        "description": "venv.Scripts.activate_this",
        "peekOfCode": "prev_length = len(sys.path)\nfor lib in '..\\\\Lib\\\\site-packages'.split(os.pathsep):\n    path = os.path.realpath(os.path.join(bin_dir, lib))\n    site.addsitedir(path.decode(\"utf-8\") if '' else path)\nsys.path[:] = sys.path[prev_length:] + sys.path[0:prev_length]\nsys.real_prefix = sys.prefix\nsys.prefix = base",
        "detail": "venv.Scripts.activate_this",
        "documentation": {}
    },
    {
        "label": "sys.path[:]",
        "kind": 5,
        "importPath": "venv.Scripts.activate_this",
        "description": "venv.Scripts.activate_this",
        "peekOfCode": "sys.path[:] = sys.path[prev_length:] + sys.path[0:prev_length]\nsys.real_prefix = sys.prefix\nsys.prefix = base",
        "detail": "venv.Scripts.activate_this",
        "documentation": {}
    },
    {
        "label": "sys.real_prefix",
        "kind": 5,
        "importPath": "venv.Scripts.activate_this",
        "description": "venv.Scripts.activate_this",
        "peekOfCode": "sys.real_prefix = sys.prefix\nsys.prefix = base",
        "detail": "venv.Scripts.activate_this",
        "documentation": {}
    },
    {
        "label": "sys.prefix",
        "kind": 5,
        "importPath": "venv.Scripts.activate_this",
        "description": "venv.Scripts.activate_this",
        "peekOfCode": "sys.prefix = base",
        "detail": "venv.Scripts.activate_this",
        "documentation": {}
    }
]